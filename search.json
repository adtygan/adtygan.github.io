[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Notes on RoFormer: Enhanced Transformer with Rotary Position Embedding\n\n\n\n\n\nDeep dive into the paper behind RoPE\n\n\n\n\n\nJun 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding Business Analyst Agents using Swarms\n\n\n\n\n\nAgentic workflows for automatic business case report generation\n\n\n\n\n\nMay 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nNever Miss a New File with fswatch\n\n\n\n\n\nIntroduction to file change monitoring using fswatch\n\n\n\n\n\nApr 1, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pages/education.html",
    "href": "pages/education.html",
    "title": "Education",
    "section": "",
    "text": "(The formal ones may be)\n\nB.Tech in IT from Netaji Subhash Engineering College (2013 - 17) (Final year dissertation: A CFS‚ÄìDNN-Based Intrusion Detection System)\nHigh School from Jadavpur Vidyapith (PCMC) (2005 - 13)\nCourses/certifications relevant to my subject of interest:\n\nData Scientist with Python Track (DataCamp)\nData Analyst with Python Track (DataCamp)\nDeep Learning Specialization (Coursera)\nAdvanced Machine Learning with TensorFlow on Google Cloud Platform (Coursera)\nTensorFlow in Practice Specialization (Coursera)\nTensorFlow: Data and Deployment Specialization (Coursera)\nGenerative Adversarial Networks (GANs) (Coursera)\nNatural Language Processing (Coursera)\nTensorFlow Developer Certification\nMathematics for Machine Learning (Coursera)\nMachine Learning Engineering for Production (MLOps) (Coursera)\nProfessional Machine Learning Engineer (Google Cloud)"
  },
  {
    "objectID": "pages/authoring.html",
    "href": "pages/authoring.html",
    "title": "Authoring",
    "section": "",
    "text": "Co-authored a book Hands-On Python Deep Learning for the Web with Anubhav Singh.\nAuthored the following liveProjects with Manning:\nAuthored two DataCamp Projects (Predicting Credit Card Approvals and Analyze International Debt Statistics) and a DataCamp Practice Pool on Advanced Deep Learning with Keras.\nBelow are the blogs, articles, and tutorials I have written on Data Science, Machine Learning, and more. I am fortunate enough to collaborate with amazing folks from all around the globe. I am grateful to the GDE Program (ML Developer Programs team) that provides me with Google Cloud Platform credits which I use to run various experiments for my own curiosity and for blog posts."
  },
  {
    "objectID": "pages/authoring.html#footnotes",
    "href": "pages/authoring.html#footnotes",
    "title": "Authoring",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n#TFCommunitySpotlight awarded.‚Ü©Ô∏é\n#TFCommunitySpotlight awarded.‚Ü©Ô∏é\n#TFCommunitySpotlight awarded.‚Ü©Ô∏é\n#TFCommunitySpotlight awarded.‚Ü©Ô∏é\nThis article got featured in ‚ÄúPython Top 10 Articles for the Past Month (v.Oct 2018)‚Äù and secured a rank of 4.‚Ü©Ô∏é\nThis article got featured in ‚ÄúMachine Learning Top 10 Articles for the Past Month (v.Nov 2018)‚Äù and secured a rank of 9.‚Ü©Ô∏é\nThis article got featured in ‚ÄúPython Top 10 Articles for the Past Month (v.Dec 2018)‚Äù and secured a rank of 10.‚Ü©Ô∏é\nFeatured in Sebastian Ruder‚Äôs monthly newsletter.‚Ü©Ô∏é\nThis one ranked eighth at a blogging competition.‚Ü©Ô∏é\n#TFCommunitySpotlight awarded.‚Ü©Ô∏é\nIn collaboration with the Neural Structured Learning team at Google.‚Ü©Ô∏é\n#TFCommunitySpotlight awarded.‚Ü©Ô∏é\n#TFCommunitySpotlight awarded.‚Ü©Ô∏é\nThis one won the ML GDE Dev Challenge.‚Ü©Ô∏é\n#TFCommunitySpotlight awarded.‚Ü©Ô∏é\n#TFCommunitySpotlight awarded.‚Ü©Ô∏é"
  },
  {
    "objectID": "pages/research.html",
    "href": "pages/research.html",
    "title": "Research",
    "section": "",
    "text": "I am interested in the area of representation learning. More recently, I have developed an interest in diffusion models.\nPlease refer to my Google Scholar profile for details on the publications I have been a part of.\n\nTutorials\n\nPractical Adversarial Robustness in Deep Learning: Problems and Solutions (CVPR‚Äô21)\nFoundational Robustness of Foundation Models (NeurIPS‚Äô22)\nAll Things ViTs: Understanding and Interpreting Attention in Vision (CVPR‚Äô23)\n\n\n\nInvited talks, demos, etc.\n\nControlling Text-to-Image Diffusion Models, BigMAC ICCV Workshop (October 02, 2023). Slides are here. Recording is here.\nDemo of üß® diffusers at ICCV 2023 (Tweet).\n\n\n\nTeaching assistance\nServed as a TA for Full Stack Deep Learning‚Äôs 2022 cohort.\n\n\nReviewing\nML Safety Workshop (NeurIPS‚Äô22), AAAI‚Äô23, UDL workshop (ICML‚Äô21), ICASSP‚Äô21 (sub-reviewer), Artificial Intelligence (Elsevier), IEEE Access.\n\n\nMisc\n\nReleased a dataset for large-scale multi-label text classification (joint work with Soumik Rakshit).\nExploration on instruction-tuning Stable Diffusion."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Aditya Ganesh Kumar",
    "section": "",
    "text": "Greetings üëã, I‚Äôm Aditya, an AI practitioner and tinkerer. I recently graduated from the National University of Singapore where I was advised by Prof.¬†Lee Wee Sun, and worked on improving text-to-image retrieval systems for product matching.\nI‚Äôm deeply passionate about Human Centered AI and believe in an exciting future where AI increasingly augments human creativity. I‚Äôll be using this space to document the interesting projects I cook up.\n\n\n\n\nThe structure of this website is inspired by Sayak‚Äôs site"
  },
  {
    "objectID": "posts/rope-notes.html#absolute-positional-embedding",
    "href": "posts/rope-notes.html#absolute-positional-embedding",
    "title": "Notes on RoFormer: Enhanced Transformer with Rotary Position Embedding",
    "section": "Absolute Positional Embedding",
    "text": "Absolute Positional Embedding\n\nInput embeddings fed to transformer only convey meaning of token in isolation\nWe want model to treat nearby words as nearby and distant words as distant\n\nImportant because self-attention is position invariant\n\nRepresent the above in a manner that model can understand\nPE values are only computed once and reused for¬†every sentence¬†during both training and inference\nAll we have to do is add these PE values with input embeddings, which change for every sentence\n\n\n\n\nSource: Umar Jamil, Attention is all you need (Transformer)\n\n\n\\[EncoderInput[i]= Embedding[i] + PositionalEmbedding[i]\\]\n\nüìù¬†Formulation\n\\[f_{t: t \\in\\{q, k, v\\}}(\\boldsymbol{x}_i, i):= \\\\\n\\boldsymbol{W}_{t: t \\in\\{q, k, v\\}}(\\boldsymbol{x}_i, \\boldsymbol{p_i})\\]\n\\[\\begin{cases} \\\\\n\\mathbf{p}_{i, 2t} = \\sin \\left( k/{10000^{2t/d}} \\right) \\\\\n\\mathbf{p}_{i, 2t+1} = \\cos \\left( k/{10000^{2t/d}} \\right)\n\\end{cases}\\]\n\n\n\nSource: Umar Jamil, Attention is all you need (Transformer)\n\n\n\n\n\n\n\n\n‚úçüèª Example\n\n\n\n\n\n\n\\(PE(1,3)\\) pertains to the second token in the sentence and the fourth value of the token embedding\nHence we use, \\(pos=1\\), \\(2i=2\\) and \\(cos\\)\n\n\n\n\n\n\nüñºÔ∏è¬†Visual Intuition\n\n\n\nSource: Amirhossein Kazemnejad, Transformer Architecture: The Positional Encoding\n\n\n\nEach row is a relative position embedding (there are 50 token positions here)\nCloser positions have visually similar encodings than farther ones\n\n\n\nüü° Limitations\n\nSequence Length Bottleneck: We cannot represent PE values (and thereby, distance) beyond the sequence length we set (50 in the above diagram)\nLack of Relative Positions: PE values are computed independently, hence, model treats pos 1 vs pos 2 the same way as pos 2 vs pos 100"
  },
  {
    "objectID": "posts/rope-notes.html#relative-positional-embedding-t5-model",
    "href": "posts/rope-notes.html#relative-positional-embedding-t5-model",
    "title": "Notes on RoFormer: Enhanced Transformer with Rotary Position Embedding",
    "section": "Relative Positional Embedding (T5 model)",
    "text": "Relative Positional Embedding (T5 model)\n\n\n\nSource: Efficient NLP, Rotary Positional Embeddings: Combining Absolute and Relative\n\n\n\nInstead of representing a token‚Äôs position, represent a distance offset between 2 tokens using trainable parameters\n\n(e.g.) \\(b_2\\) represents distance offset of 2\n\n\n\nOffset/Bias Matrix\n\n\n\n\\(Anthony\\)\n\\(Hopkins\\)\n\\(admired\\)\n\n\n\n\n\\(Anthony\\)\n\\(b_0\\)\n\\(b_1\\)\n\\(b_2\\)\n\n\n\\(Hopkins\\)\n\\(b_{-1}\\)\n\\(b_0\\)\n\\(b_1\\)\n\n\n\\(admired\\)\n\\(b_{-2}\\)\n\\(b_{-1}\\)\n\\(b_0\\)\n\n\n\n\nToken pairs that are same distance apart have same offset value\nThis offset matrix (called bias) is added to attention matrix to encode positions\n\n\\[Final Attention Matrix = Attention Matrix + Offset Matrix\\]\n\nüìù¬†Formulation\nFormulation for Absolute Position Embedding:\n\\[f_{t : t \\in \\{q, k, v\\}} \\left( \\mathbf{x}_i, i \\right) := \\\\\n\\mathbf{W}_{t : t \\in \\{q, k, v\\}} \\left( \\mathbf{x}_i + \\mathbf{p}_i \\right)\\]\nFormulation expected for Relative Position Embedding:\n\\[\\begin{aligned}\nf_q (\\mathbf{x}_m) & := \\mathbf{W}_q \\mathbf{x}_m \\\\\nf_k (\\mathbf{x}_n, n) & := \\mathbf{W}_k (\\mathbf{x}_n + \\tilde{\\mathbf{p}}_r^k) \\\\\nf_v (\\mathbf{x}_n, n) & := \\mathbf{W}_v (\\mathbf{x}_n + \\tilde{\\mathbf{p}}_r^v)\n\\end{aligned}\n\\]\n\n\\(\\tilde{p}^k_r\\) and \\(\\tilde{p}^v_r\\) are trainable relative position embeddings\n\\(r = clip(m ‚àí n, rmin, rmax)\\) , is clipped due to hypothesis that precise relative position is not useful beyond a certain distance\nAfter a few steps, below is the form we arrive for relative position embeddings:\n\n\\[q_m^T k_n = x_m^T W_q^T W_k x_n + x_m^T W_q^T \\tilde{W}_k \\tilde{p}_{m-n} + \\\\\n\\mathrm{u}^T W_q^T W_k x_n + \\mathrm{v}^T W_q^T \\tilde{W}_k \\tilde{p}_{m-n}\\]\n\nBut, later works have simplified the form ü§ó\nFor instance, Colin Raffel et al. ‚ÄúExploring the limits of transfer learning with a unified text-to-text transformer‚Äù use:\n\n\\[q_m^T k_n = x_m^T W_q^T W_k x_n + b_{i,j}\\]\n\nThis form is similar to the one discussed for T5 model\n\n\n\nüü°¬†Limitations\n\nRe-computation of Attention Matrix: Due to this, as sequence length increases, inference speed decreases\nIssues with KV Cache: For every new token added to sequence, embeddings for all tokens will change, effectively rendering KV Cache impractical\n\n\nNotice how bias values for \\(Anthony\\), \\(Hopkins\\) and \\(admired\\) changed (1 more column added)\n\n\n\n\\(Anthony\\)\n\\(Hopkins\\)\n\\(admired\\)\n\\(Michael\\)\n\n\n\n\n\\(Anthony\\)\n\\(b_0\\)\n\\(b_1\\)\n\\(b_2\\)\n\\(b_3\\)\n\n\n\\(Hopkins\\)\n\\(b_{-1}\\)\n\\(b_0\\)\n\\(b_1\\)\n\\(b_2\\)\n\n\n\\(admired\\)\n\\(b_{-2}\\)\n\\(b_{-1}\\)\n\\(b_0\\)\n\\(b_1\\)\n\n\n\\(Michael\\)\n\\(b_{-3}\\)\n\\(b_{-2}\\)\n\\(b_{-1}\\)\n\\(b_0\\)"
  },
  {
    "objectID": "posts/rope-notes.html#enter-rotary-positional-embedding",
    "href": "posts/rope-notes.html#enter-rotary-positional-embedding",
    "title": "Notes on RoFormer: Enhanced Transformer with Rotary Position Embedding",
    "section": "üßµ¬†Enter Rotary Positional Embedding",
    "text": "üßµ¬†Enter Rotary Positional Embedding\n\nInstead of adding position embeddings to token embeddings, rotate the token embeddings based on its position in the sequence\nThis solves the limitations of its precursors\n‚úÖ¬†Solves the issue with KV Cache because we no longer compute a separate bias matrix\n\n\n\n\nSource: Efficient NLP, Rotary Positional Embeddings: Combining Absolute and Relative\n\n\n\n‚úÖ¬†Notion of relative distance is preserved across rotations\n\n\n\n\n\nSource: Efficient NLP, Rotary Positional Embeddings: Combining Absolute and Relative\n\n\n\nüìù¬†Formulation\n\nRemember:\n\n\\[q_m = f_q(x_m, m)\\]\n\\[k_n = f_k(x_n, n)\\]\n\\[v_n = f_v(x_n, n)\\]\n\\[a_{m,n} = \\frac{\\exp(\\frac{q_m^\\top k_n}{\\sqrt{d}})}{\\sum_{j=1}^N \\exp(\\frac{q_m^\\top k_j}{\\sqrt{d}})}\\]\n\\[o_m = \\sum_{n=1}^N a_{m,n}v_n\\]\n\nIn Absolute Positional Embedding method, \\(q^T_m\\) and \\(k_n\\) each encoded individual token positions\nHence, self-attention, (aka \\(q^T_mk_n\\)) led to propagation of information about individual token positions\nWe want to move away from that and only incorporate relative position information during the self-attention operation\n\nIn my understanding, it is because the absolute position is already encoded in the rotation\n\nIn other words, we want the below to happen\n\n\\(q_m = f_q(x_m, m)\\)\n\\(k_n = f_k(x_n, n)\\)\n\\(\\langle \\cdot, \\cdot \\rangle\\) denotes inner-product\n\n\n\\[q^T_m k_n = \\langle f_q(x_m, m), f_k(x_n, n) \\rangle = g(x_m, x_n, m - n)\\]\n\nNote that \\(g(\\cdot)\\) does not have any individual position information like \\(g(\\cdot, m, n)\\)\n\n\nSolution for 2D Case\n\nThe authors prove that, for the case of 2D vectors, the solution to the above problem is:\n\n\\[f_q(\\mathbf{x}_m, m) = (\\mathbf{W}_q \\mathbf{x}_m) e^{im\\theta}\\]\n\\[f_k(\\mathbf{x}_n, n) = (\\mathbf{W}_k \\mathbf{x}_n) e^{in\\theta}\\]\n\\[g(\\mathbf{x}_m, \\mathbf{x}_n, m - n) = \\text{Re}[(\\mathbf{W}_q \\mathbf{x}_m)(\\mathbf{W}_k \\mathbf{x}_n)^* e^{i(m-n)\\theta}]\\]\n\\[f_{\\{q,k\\}}(x_m, m) = \\begin{pmatrix}\n    \\cos m\\theta & -\\sin m\\theta \\\\\n    \\sin m\\theta & \\cos m\\theta\n\\end{pmatrix}\n\\begin{pmatrix}\n    W_{\\{q,k\\}}^{(11)} & W_{\\{q,k\\}}^{(12)} \\\\\n    W_{\\{q,k\\}}^{(21)} & W_{\\{q,k\\}}^{(22)}\n\\end{pmatrix}\n\\begin{pmatrix}\n    x_m^{(1)} \\\\\n    x_m^{(2)}\n\\end{pmatrix}\\]\n\nNotice how first, the Key and Query are generated before rotation\n\n\n\nSolution for General Case\n\nFor any \\(d\\) dimensional input embedding, we assume \\(d\\) is even and divide the embedding space into \\(d/2\\) subspaces\n\n\\[f_{\\{q,k\\}}(\\textbf{x}_m, m) = \\textbf{R}_{\\Theta,m}^d \\textbf{W}_{\\{q,k\\}} \\textbf{x}_m\\]\n\\[\n\\begin{equation}\n    R_{\\Theta,m}^d = \\begin{pmatrix}\n        \\cos m\\theta_1 & -\\sin m\\theta_1 & 0 & 0 & \\cdots & 0 & 0 \\\\\n        \\sin m\\theta_1 & \\cos m\\theta_1 & 0 & 0 & \\cdots & 0 & 0 \\\\\n        0 & 0 & \\cos m\\theta_2 & -\\sin m\\theta_2 & \\cdots & 0 & 0 \\\\\n        0 & 0 & \\sin m\\theta_2 & \\cos m\\theta_2 & \\cdots & 0 & 0 \\\\\n        \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n        0 & 0 & 0 & 0 & \\cdots & \\cos m\\theta_{d/2} & -\\sin m\\theta_{d/2} \\\\\n        0 & 0 & 0 & 0 & \\cdots & \\sin m\\theta_{d/2} & \\cos m\\theta_{d/2}\n    \\end{pmatrix}\n\\end{equation}\n\\]\n\nThis leads us to the general form of self-attention using RoPE\n\n\\[\\begin{equation}\n    \\textbf{q}_m^\\top \\textbf{k}_n = (\\textbf{R}_{\\Theta,m}^d \\textbf{W}_q \\textbf{x}_m)^\\top (\\textbf{R}_{\\Theta,n}^d \\textbf{W}_k \\textbf{x}_n) = \\textbf{x}_m^\\top \\textbf{W}_q^\\top \\textbf{R}_{\\Theta,n-m}^d \\textbf{W}_k \\textbf{x}_n\n\\end{equation}\\]\n\nNotice how RoPE incorporates relative position information in the self-attention step itself instead of adding it through a bias matrix like its precursor\n\n\n\n\nImplementation of RoPE, all subspaces within an embedding vector are rotated by same angle\n\n\n\n\n\nüçÄ Properties of RoPE\n\nFlexibly adapts to any sequence length\n\nFor any new token, all we need to do is rotate its vector by \\(m\\theta\\)\nThe method is scalable and requires no pre-computation\n\n\n\nDecay of the inter-token dependency\n\n\nRelative upper bound is the \\(\\textbf{R}_{\\Theta,n-m}^d\\) term in general form of RoPE: \\(\\textbf{x}_m^\\top \\textbf{W}_q^\\top \\textbf{R}_{\\Theta,n-m}^d \\textbf{W}_k \\textbf{x}_n\\)\n\n\n\nEquips linear self-attention with relative position encoding\n\nInstead of expressing self-attention in the usual way as:\n\n\\[\n\\begin{equation}\n    \\text{Attention}(\\textbf{Q}, \\textbf{K}, \\textbf{V})_m = \\frac{\\sum_{n=1}^N \\text{sim}(\\textbf{q}_m, \\textbf{k}_n) \\textbf{v}_n}{\\sum_{n=1}^N \\text{sim}(\\textbf{q}_m, \\textbf{k}_n)}\n\\end{equation}\n\\]\n\nLinear-attention expresses it as:\n\n\\[\n\\begin{equation}\n    \\text{Attention}(\\textbf{Q}, \\textbf{K}, \\textbf{V})_m = \\frac{\\sum_{n=1}^N \\phi(\\textbf{q}_m)^\\top \\phi(\\textbf{k}_n) \\textbf{v}_n}{\\sum_{n=1}^N \\phi(\\textbf{q}_m)^\\top \\phi(\\textbf{k}_n)}\n\\end{equation}\n\\]\n\nThe authors propose that RoPE can be combined with linear-attention simply by multiplying the rotation matrices\n\n\\[\n\\begin{equation}\n    \\text{Attention}(\\textbf{Q}, \\textbf{K}, \\textbf{V})_m = \\frac{\\sum_{n=1}^N (\\textbf{R}_{\\Theta,m}^d \\phi(\\textbf{q}_m))^\\top (\\textbf{R}_{\\Theta,n}^d \\phi(\\textbf{k}_n)) \\textbf{v}_n}{\\sum_{n=1}^N \\phi(\\textbf{q}_m)^\\top \\phi(\\textbf{k}_n)}\n\\end{equation}\n\\]"
  },
  {
    "objectID": "posts/rope-notes.html#results",
    "href": "posts/rope-notes.html#results",
    "title": "Notes on RoFormer: Enhanced Transformer with Rotary Position Embedding",
    "section": "Results",
    "text": "Results\n\n1. Machine Translation: WMT 2014 English -German Translation\n\nEvaluate English-to-German translation\n\n\n\n\n2. Pre-training Language Modeling\n\nValidate performance in learning contextual representations\nPre-training dataset: BookCorpus and Wikipedia Corpus Foundation\n\n\n\n\nMLM Loss - Masked Language Model Loss, (Cross-Entropy Loss)\n\n\n\n\n3. PerFormer + RoPE for Pre-Training\n\nPerFormer uses linear-attention\n\n\n\n\nLM Loss - Language Model Loss, (Cross-Entropy Loss)\n\n\n\n\n4. Fine-tuning on GLUE tasks\n\nGLUE (General Language Understanding Evaluation): Benchmark for evaluating performance of natural language understanding (NLU) models across a diverse set of tasks\n\n\n\n\nEach dataset uses different metric(s), hence, scores are averaged to a final score\n\n\n\n\n5. Handling long texts on Chinese Datasets\n\nPerformance improves as sequence length increases\n\n\n\n\n\nüü°¬†Limitations of RoPE\n\nUnclear why RoPE converges faster than other strategies\nSuperior performance on long texts cannot be explained convincingly\n\nAttributed to long-term decay, but other strategies have this property as well"
  },
  {
    "objectID": "posts/rope-notes.html#rope-scaling",
    "href": "posts/rope-notes.html#rope-scaling",
    "title": "Notes on RoFormer: Enhanced Transformer with Rotary Position Embedding",
    "section": "RoPE Scaling",
    "text": "RoPE Scaling\n\nRoPE has seen widespread adoption in modern architectures like LLaMA and Gemma\nOne exciting reason is RoPE Scaling\nMain Idea: Modify the base value (10,000 by default) to allow model to handle longer sequences\nBase value is scaled using rope_factor\n\nrope_factor = 2 doubles context length\nFinding optimal base value depends on task, architecture and requires experimentation\n\nIdea discussed better in Xiaoran Liu, et al., Scaling Laws of RoPE-based Extrapolation\nJust by scaling base value, RoPE can be used for longer sequences, but performance degrades\n\n(e.g.) Going from 4k to 32k without finetuning has been done\n\nFinetuning post scaling helps tackle this issue\nAnother idea is to progressively scale and finetune to drastically improve context length\n‚ú®‚ú®Scaling from 8k to 2048k with finetuning has been achieved\n\n\n\n\nSource: Yiran Ding, et al., LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens"
  },
  {
    "objectID": "posts/rope-notes.html#references",
    "href": "posts/rope-notes.html#references",
    "title": "Notes on RoFormer: Enhanced Transformer with Rotary Position Embedding",
    "section": "üìã¬†References",
    "text": "üìã¬†References\n\nJianlin Su, et al., RoFormer: Enhanced Transformer with Rotary Position Embedding\nUmar Jamil, Attention is all you need (Transformer)\nAmirhossein Kazemnejad, Transformer Architecture: The Positional Encoding\nEfficient NLP, Rotary Positional Embeddings: Combining Absolute and Relative"
  },
  {
    "objectID": "posts/fswatch.html#context",
    "href": "posts/fswatch.html#context",
    "title": "Never Miss a New File with fswatch",
    "section": "Context",
    "text": "Context\nAs someone with a new found interest in automating things, I have been working on building a local image search engine called WarpSearch. It will allow me to query locally stored images using text and this is amazing because I can dump all my images in one folder and be very hopeful that I will be able to find it later. But folders don‚Äôt remain the same. Images get added or deleted and I need to keep track of these changes."
  },
  {
    "objectID": "posts/fswatch.html#enter-fswatch",
    "href": "posts/fswatch.html#enter-fswatch",
    "title": "Never Miss a New File with fswatch",
    "section": "Enter fswatch",
    "text": "Enter fswatch\nThis is where fswatch comes in clutch. It‚Äôs a tool that allows monitoring a folder for changes. And in this post, I want to cover a general recipe that allows for detecting file additions and deletions in a folder.\nLet‚Äôs call the script file-monitor.sh. It monitors the folder path ~/Vault and tracks files with extensions .jpg, .jpeg and .png. Both the folder path and file extensions tracked are easy to modify for different scenarios. Below is the code:\nFOLDER=~/Vault\n\n# Define a function to get state of directory\nget_state() {\n  find $FOLDER -type f \\( -iname \"*.jpg\" -o -iname \"*.jpeg\" -o -iname \"*.png\" \\)\n}\n\n# Initialize previous_state with state of directory\nprevious_state=$(get_state)\n\n# Monitor the ~/Vault directory for changes\nfswatch -0 -o $FOLDER | while read -d \"\" event; do\n  # For each event, list current state of directory \n  current_state=$(get_state)\n  # Diff current state with previous state\n  diff &lt;(echo \"$previous_state\") &lt;(echo \"$current_state\") | while read line; do\n    if [[ $line == \"&gt;\"* ]]; then\n      echo \"Added: ${line:2}\"\n    elif [[ $line == \"&lt;\"* && ! -z \"${line:2}\" ]]; then\n      echo \"Deleted: ${line:2}\"\n    fi\n  done\n  previous_state=$current_state\ndone\nThe code works by comparing the current state of the folder with its previous state. This allows it to detect file additions and deletions. After starting the script with bash file-monitor.sh, I added 2 images and renamed 1 exising image. Here is the output it produces:\nAdded: /Users/aditya/Vault/Orange Boat.jpg\nAdded: /Users/aditya/Vault/Futuristic Red.jpg\nDeleted: /Users/aditya/Vault/Orange Boat.jpg\nAdded: /Users/aditya/Vault/Orange Boat Illustration.jpg\nAs you can see, the script handles renaming as a combination of deletion and addition. This is a limitation of the chosen approach. A fix that I haven‚Äôt explored involves the following: When a Deletion + Addition pair is detected, check if the file count remains same before and after the pair of operations. If so, it is a Rename operation."
  },
  {
    "objectID": "posts/fswatch.html#running-as-background-process",
    "href": "posts/fswatch.html#running-as-background-process",
    "title": "Never Miss a New File with fswatch",
    "section": "Running as Background Process",
    "text": "Running as Background Process\nIn most cases, we want the script to run in the background, even after we have closed the terminal. To do this, we first give the script permission to execute as a program.\nchmod +x ./file-monitor.sh       \nThen, we use nohup to run the process in detached mode and store the outputs in a separate log file.\nnohup ./file-monitor.sh &gt; fswatch.log"
  },
  {
    "objectID": "posts/fswatch.html#closing-thoughts",
    "href": "posts/fswatch.html#closing-thoughts",
    "title": "Never Miss a New File with fswatch",
    "section": "Closing Thoughts",
    "text": "Closing Thoughts\nThat wraps it up for my first mini-blog. I hope this post encourages you to check out fswatch and try out this recipe for your own use cases. If you have any questions or suggestions, drop your comments below or please reach out to me on Twitter."
  },
  {
    "objectID": "posts/swarms-business-agent.html",
    "href": "posts/swarms-business-agent.html",
    "title": "Building Business Analyst Agents using Swarms",
    "section": "",
    "text": "Solving a business problem often involves preparing a Business Case Report. This report comprehensively analyzes the problem, evaluates potential solutions, and provides evidence-based recommendations and an implementation plan to effectively address the issue and drive business value. While the process of preparing one requires an experienced business analyst, the workflow can be augmented using AI agents. Two candidates stick out as areas to work on:\nIn this blog, we will explore how Swarms agents can be used to tackle a busuiness problem by outlining the solution, conducting background research and generating a preliminary report."
  },
  {
    "objectID": "posts/swarms-business-agent.html#final-generated-report",
    "href": "posts/swarms-business-agent.html#final-generated-report",
    "title": "Building Business Analyst Agents using Swarms",
    "section": "Final Generated Report",
    "text": "Final Generated Report\n\nNike‚Äôs Current Revenue Trend\nNike‚Äôs current revenue trend has been steadily increasing over the past few years. In the most recent fiscal year, Nike reported a revenue of $37.4 billion, which was a 7% increase from the previous year. This growth can be attributed to strong sales in key markets, successful marketing campaigns, and a focus on innovation in product development. Overall, Nike continues to demonstrate strong financial performance and is well-positioned for future growth.\n\n\nPotential Areas of Improvement in Nike‚Äôs Business Model\n\nSustainability Practices: Nike could further enhance its sustainability efforts by reducing its carbon footprint, using more eco-friendly materials, and ensuring ethical labor practices throughout its supply chain.\nDiversification of Product Portfolio: While Nike is known for its athletic footwear and apparel, diversifying into new product categories or expanding into untapped markets could help drive growth and mitigate risks associated with a single product line.\nE-commerce Strategy: Improving the online shopping experience, investing in digital marketing, and leveraging data analytics to personalize customer interactions could boost online sales and customer loyalty.\nInnovation and R&D: Continuously investing in research and development to stay ahead of competitors, introduce new technologies, and enhance product performance could help maintain Nike‚Äôs competitive edge in the market.\nBrand Image and Reputation: Strengthening brand image through effective marketing campaigns, community engagement, and transparent communication with stakeholders can help build trust and loyalty among consumers.\n\n\n\nPotential Cost-Saving Strategies for Nike to Increase Net Revenue in Q3 2024\n\nSupply Chain Optimization: Streamlining the supply chain, reducing transportation costs, and improving inventory management can lead to significant cost savings for Nike.\nOperational Efficiency: Implementing lean manufacturing practices, reducing waste, and optimizing production processes can help lower production costs and improve overall efficiency.\nOutsourcing Non-Core Functions: Outsourcing non-core functions such as IT services, customer support, or logistics can help reduce overhead costs and focus resources on core business activities.\nEnergy Efficiency: Investing in energy-efficient technologies, renewable energy sources, and sustainable practices can lower utility costs and demonstrate a commitment to environmental responsibility.\nNegotiating Supplier Contracts: Negotiating better terms with suppliers, leveraging economies of scale, and exploring alternative sourcing options can help lower procurement costs and improve margins.\n\nBy implementing these cost-saving strategies, Nike can improve its bottom line and increase net revenue in Q3 2024.\n\n\nProjected Market Trends for the Sports Apparel Industry in 2024\n\nSustainable Fashion: Consumers are increasingly demanding eco-friendly and sustainable products, leading to a rise in sustainable sportswear options in the market.\nDigital Transformation: The sports apparel industry is expected to continue its shift towards digital platforms, with a focus on e-commerce, personalized shopping experiences, and digital marketing strategies.\nAthleisure Wear: The trend of athleisure wear, which combines athletic and leisure clothing, is projected to remain popular in 2024 as consumers seek comfort and versatility in their apparel choices.\nInnovative Materials: Advances in technology and material science are likely to drive the development of innovative fabrics and performance-enhancing materials in sports apparel, catering to the demand for high-quality and functional products.\nHealth and Wellness Focus: With a growing emphasis on health and wellness, sports apparel brands are expected to incorporate features that promote comfort, performance, and overall well-being in their products.\n\nOverall, the sports apparel industry in 2024 is anticipated to be characterized by sustainability, digitalization, innovation, and a focus on consumer health and lifestyle trends.\n\n\nCurrent Successful Strategies Used by Nike‚Äôs Competitors\n\nAdidas: Adidas has been successful in leveraging collaborations with celebrities and designers to create limited-edition collections that generate hype and drive sales. They have also focused on sustainability initiatives, such as using recycled materials in their products, to appeal to environmentally conscious consumers.\nUnder Armour: Under Armour has differentiated itself by targeting performance-driven athletes and emphasizing technological innovation in their products. They have also invested heavily in digital marketing and e-commerce to reach a wider audience and enhance the customer shopping experience.\nPuma: Puma has successfully capitalized on the athleisure trend by offering stylish and versatile sportswear that can be worn both in and out of the gym. They have also focused on building partnerships with influencers and sponsoring high-profile athletes to increase brand visibility and credibility.\nLululemon: Lululemon has excelled in creating a strong community around its brand, hosting events, classes, and collaborations to engage with customers beyond just selling products. They have also prioritized customer experience by offering personalized services and creating a seamless omnichannel shopping experience.\nNew Balance: New Balance has carved out a niche in the market by emphasizing quality craftsmanship, heritage, and authenticity in their products. They have also focused on customization and personalization options for customers, allowing them to create unique and tailored footwear and apparel.\n\nOverall, Nike‚Äôs competitors have found success through a combination of innovative product offerings, strategic marketing initiatives, and a focus on customer engagement and experience.\n\n\nCurrent and Projected Economic Conditions in Nike‚Äôs Major Markets\n\nUnited States: The United States, being one of Nike‚Äôs largest markets, is currently experiencing moderate economic growth driven by consumer spending, low unemployment rates, and a rebound in manufacturing. However, uncertainties surrounding trade policies, inflation, and interest rates could impact consumer confidence and spending in the near future.\nChina: China remains a key market for Nike, with a growing middle class and increasing demand for sportswear and athletic footwear. Despite recent trade tensions with the U.S., China‚Äôs economy is projected to continue expanding, driven by domestic consumption, infrastructure investments, and technological advancements.\nEurope: Economic conditions in Europe vary across countries, with some experiencing sluggish growth due to Brexit uncertainties, political instability, and trade tensions. However, overall consumer confidence is improving, and the sports apparel market is expected to grow, driven by e-commerce and sustainability trends.\nEmerging Markets: Nike‚Äôs presence in emerging markets such as India, Brazil, and Southeast Asia provides opportunities for growth, given the rising disposable incomes, urbanization, and increasing focus on health and fitness. However, challenges such as currency fluctuations, regulatory changes, and competition from local brands could impact Nike‚Äôs performance in these markets.\n\nOverall, Nike‚Äôs major markets exhibit a mix of opportunities and challenges, with economic conditions influenced by global trends, geopolitical factors, and consumer preferences.‚Äù\n\n\nCurrent Consumer Preferences in the Sports Apparel Industry\n\nSustainability: Consumers are increasingly seeking eco-friendly and sustainable options in sports apparel, driving brands to focus on using recycled materials, reducing waste, and promoting ethical practices.\nAthleisure: The trend of athleisure wear continues to be popular, with consumers looking for versatile and comfortable clothing that can be worn both during workouts and in everyday life.\nPerformance and Functionality: Consumers prioritize performance-enhancing features in sports apparel, such as moisture-wicking fabrics, breathable materials, and ergonomic designs that enhance comfort and mobility.\nPersonalization: Customization options, personalized fit, and unique design elements are appealing to consumers who seek individuality and exclusivity in their sports apparel choices.\nBrand Transparency: Consumers value transparency in brand practices, including supply chain transparency, ethical sourcing, and clear communication on product quality and manufacturing processes.\n\nOverall, consumer preferences in the sports apparel industry are shifting towards sustainability, versatility, performance, personalization, and transparency, influencing brand strategies and product offerings.\n\n\nPotential New Markets for Nike to Explore in 2024\n\nIndia: With a growing population, increasing disposable incomes, and a rising interest in health and fitness, India presents a significant opportunity for Nike to expand its presence and tap into a large consumer base.\nAfrica: The African market, particularly countries with emerging economies and a young population, offers potential for Nike to introduce its products and capitalize on the growing demand for sportswear and athletic footwear.\nMiddle East: Countries in the Middle East, known for their luxury shopping destinations and a growing interest in sports and fitness activities, could be strategic markets for Nike to target and establish a strong foothold.\nLatin America: Markets in Latin America, such as Brazil, Mexico, and Argentina, present opportunities for Nike to cater to a diverse consumer base and leverage the region‚Äôs passion for sports and active lifestyles.\nSoutheast Asia: Rapid urbanization, increasing urban middle-class population, and a trend towards health and wellness in countries like Indonesia, Thailand, and Vietnam make Southeast Asia an attractive region for Nike to explore and expand its market reach.\n\nBy exploring these new markets in 2024, Nike can diversify its geographical presence, reach untapped consumer segments, and drive growth in emerging economies.\n\n\nPotential New Products or Services Nike Could Introduce in 2024\n\nSmart Apparel: Nike could explore the integration of technology into its apparel, such as smart fabrics that monitor performance metrics, provide feedback, or enhance comfort during workouts.\nAthletic Accessories: Introducing a line of athletic accessories like gym bags, water bottles, or fitness trackers could complement Nike‚Äôs existing product offerings and provide additional value to customers.\nCustomization Platforms: Offering personalized design options for footwear and apparel through online customization platforms could appeal to consumers seeking unique and tailored products.\nAthletic Recovery Gear: Developing recovery-focused products like compression wear, recovery sandals, or massage tools could cater to athletes and fitness enthusiasts looking to enhance post-workout recovery.\nSustainable Collections: Launching sustainable collections made from eco-friendly materials, recycled fabrics, or biodegradable components could align with consumer preferences for environmentally conscious products.\n\nBy introducing these new products or services in 2024, Nike can innovate its product portfolio, cater to evolving consumer needs, and differentiate itself in the competitive sports apparel market.\n\n\nPotential Marketing Strategies for Nike to Increase Revenue in Q3 2024\n\nInfluencer Partnerships: Collaborating with popular athletes, celebrities, or social media influencers to promote Nike products can help reach a wider audience and drive sales.\nInteractive Campaigns: Launching interactive marketing campaigns, contests, or events that engage customers and create buzz around new product releases can generate excitement and increase brand visibility.\nSocial Media Engagement: Leveraging social media platforms to connect with consumers, share user-generated content, and respond to feedback can build brand loyalty and encourage repeat purchases.\nLocalized Marketing: Tailoring marketing messages, promotions, and product offerings to specific regions or target demographics can enhance relevance and appeal to diverse consumer groups.\nCustomer Loyalty Programs: Implementing loyalty programs, exclusive offers, or rewards for repeat customers can incentivize brand loyalty, increase retention rates, and drive higher lifetime customer value.\n\nBy employing these marketing strategies in Q3 2024, Nike can enhance its brand presence, attract new customers, and ultimately boost revenue growth."
  },
  {
    "objectID": "pages/xyz.html",
    "href": "pages/xyz.html",
    "title": "XYZ",
    "section": "",
    "text": "On this page, I have listed some things that I do/used to do in the pursuit of ‚Äúhappyness‚Äù.\n\nMisc\n\nTaught under-privileged children and managed operations for a TCS-CSR initiative called H20 (Helping Hand Organization) (2017)\nModerator of the Artificial Intelligence channel of Campus Commune (Jan, 2018 - August, 2018)\nBook reviewer for Manning Publications Co (2019 - 2022)\nCo-organizer of GDG Kolkata (2019 - 2021)\nOrganizer of TensorFlow User Group Kolkata (2020 - 2021)\n\n\n\nMentorship\n\nLaunchpad Women Entrepreneurs (2019)\nExplore ML Academy, Hyderabad (2019)\nAI Hack Tunisia (2019)\nClass III (2019) of Launchpad Accelerator India (Tweet by GoogleDevsIN)\nGoogle Code-in for TensorFlow through December, 2019 - January 2020 (Certificate)\nBuild For Digital India Bootcamps through January 2020 - February 2020 (Tweet by GoogleDevsIN)\nExplore ML Bootcamp, Hyderabad (2020) (Tweet by GoogleDevsIN)\nGoogle for Startups MENA Accelerator 2020 (this was formerly known as Launchpad)\nGoogle for Startups India Accelerator 2020, 2021\nMentored for TensorFlow at Google Summer of Code 2021 (GSoC). Students: Aditya Kane and Vasudev Gupta. Here‚Äôs a related blog post from TensorFlow.\nDeep Learning Indaba. Various university students from Africa since the mentorship program is Africa-focused (2021- 2022).\nMentored for TensorFlow at GSoC 2022. Students: Aditya Kane, Sayan Nath, and Mohammad Shoaib. My mentorship certificate is here.\nGoogle for Startups India Women Founders Accelerator (2022).\nMentored at KaggleX BIPOC 2023. Here is my certificate.\n\nApart from the occasions mentioned above, I have mentored individual students, students belonging to Google Developer Student Clubs, etc. But I don‚Äôt have any recorded documents for these. I enjoy sharing my learnings with those in need and, in turn, learning from them.\n\n\nAwards and recognition\n\nRecipient of the Google Open Source Peer Bonus Award (2020, 2021, and 2022). Here‚Äôs a related blog post (from the official Google Open Source Blog) jotting down the experiences that led to this honor (2020).\nLed our team at PyImageSearch to Top 10 at this CVPR 2021 competition.\nFinished 2nd (with Siddha Ganju) at this competition (2021) organized by the NASA Impact team (Certificate).\nReceived (sometimes jointly) the #TFCommunitySpotlight award multiple times for projects with varying use-cases, tooling, and motivation. Details in the following tweets by TensorFlow: Tweet 1, Tweet 2, Tweet 3, Tweet 4, Tweet 5, Tweet 6, Tweet 7, Tweet 8, Tweet 9, Tweet 10, and Tweet 11.\nRanked 8 at Applied Roots Blogathon 2021.\nRecipient of the Intel Top Innovator Award (2019).\nBecame a Google Cloud Innovator (Champion tier).\nAwarded as the Top Regional Contributor from India for TensorFlow, 2021. Blog post and Tweet from TensorFlow.\nReceived NeurIPS 2021 AI4Science Workshop Travel Grant for this work.\nReceived the Google OSS Expert Prize (February, 2022) with Soumik Rakshit for this work.\nReceived the Google OSS Expert Prize (March, 2022) with Aritra Roy Gosthipaty for this work.\nReceived the Google OSS Expert Prize (April, 2022) for this work.\nReceived the Google OSS Expert Prize (May, 2022) with Aritra Roy Gosthipaty for this work.\nAwarded as a Top Contributor to Papers With Code. Award link is here.\nAccepted to the Oxford ML Summer School (2022) for the healthcare track. Certificate link is here.\nReceived the Kaggle ML Research Spotlight Prize (September, 2022) for this work.\nReceived the Kaggle ML Research Spotlight Prize (November, 2022) for this work.\nSecured 2nd place in the first ever Keras Community Prize Competition organized by Google for this project with Chansung Park.\n\n\n\nNon-tech\n\nI love listening to all genres of music. A guitar player myself. Have played in a band Behest from 2013 to 2017.\nI love watching TV serials also (Narcos, Suits, Downton Abbey, Breaking Bad, Ozark being all-time favorites)."
  },
  {
    "objectID": "pages/talksseminarsworkshops.html",
    "href": "pages/talksseminarsworkshops.html",
    "title": "Talks/Seminars/Workshops",
    "section": "",
    "text": "I love to attend developer meetups, conferences, workshops and learn from them as much as I can. I sometimes talk on a range of topics that I love the most. All the slides of my talks/sessions can be found below.\n\nGiven by me\n\nPresented our paper A CFS‚ÄìDNN-Based Intrusion Detection System at International Conference on Communication Devices and Networking, Sikkim Manipal Institute of Technology, Sikkim, June 3, 2017.\nPresented our paper A Comparative Study of Different Ensemble Learning Techniques Using Wisconsin Breast Cancer Dataset, at International Conference on Computer, Electrical & Communication Engineering, Techno India University, Kolkata, December 23, 2017.\nCo-presented our paper A Novel Transfer Learning-Based Missing Value Imputation on Discipline Diverse Real Test Datasets‚ÄîA Comparative Study with Different Machine Learning Algorithms at International Conference on Emerging Technologies in Data Mining and Information Security, University of Engineering and Management, Kolkata, February 23, 2018.\nSpoke on Cyclical Learning Rates for training Neural Nets at DevFest Kolkata, November 3, 2018.\nConducted a hack-session on Cyclical Learning Rates at DataHack Summit (organized by Analytics Vidhya), Bangalore, November 23, 2018.\nDelivered talks on Introduction to BigQuery at GDG Kolkata Cloud Study Jam (Academy of Technology), Google Cloud Next ‚Äô19 Extended - Kolkata on April 12 and April 19, 2019 respectively.\nConducted a session on Ten Updates Introduced in TensorFlow 2.0 along with a short quiz at Google I/O Extended 2019, Kolkata, May 11, 2019.\nConducted a session on Training neural nets: A methodical approach at ML/AR Developer Day organized by GDG Kolkata and DSC HIT (May 30, 2019).\n\nConducted the same session but in a more detailed manner at ML With The Experts - GDG Kolkata Meetup (July 7, 2019).\n\nSpoke at Google I/O Extended 2019, Bhubaneswar on Ten Updates Introduced in TensorFlow 2.0, June 9, 2019. Also shared a few opportunities with the students (link to the Opportunities‚Äô deck).\nSpoke at DevFest Kolkata 2019 about how to approach the process of model deployment, August 3, 2019_._ My talk was titled Connecting Flutter with TensorFlow 2.0. Link to the slides, video and the GitHub repository.\nSpoke at DevFest Jaipur 2019 (September 08, 2019) on Structuring Machine Learning Projects.\n\nRemotely presented on this topic at DevFest Izmir 2019 (November 23, 2019). Here‚Äôs the modified deck. Here‚Äôs a recording of the session.\n\nSpoke at Explore ML Academy on Problem Framing and How to find data set and fairness practices, September 14, 2019, Hyderabad.\nSpoke at DevFest Bhubaneswar 2019 (September 22, 2019) on The Human Loop in Machine Learning.\nSpoke at DevFest Goa 2019 (September 29, 2019) on Training Neural Nets: a Hacker‚Äôs Perspective.\n\nSpoke at Class III of Launchpad Accelerator India (October 16, 2019), Bangalore on an extended version of the same topic. Deck: http://bit.ly/LPA_3.\nRemotely presented on this topic at DevFest Warsaw & Radzymin 2019 (December 7, 2019).\n\nPresented my work on Blood Cell Detection using TensorFlow Object Detection API at TensorFlow Roadshow, Bangalore (October 01, 2019). Deck: http://bit.ly/tf-roadshow-sayak.\nRemotely presented my work on Predicting Publisher‚Äôs Names from Hackernews Article Titles at Global GDE Summit (October 26, 2019). Video available here (Courtesy: Akshay Bahadur). Deck: http://bit.ly/GDESummit19.\nRemotely presented at Machine Learning Weekend, Turkey on Building data pipelines with tf.data (November 3, 2019).\nPresented at Kaggle Days Mumbai on On the learning dynamics of neural nets (November 30, 2019).\nConducted a workshop on Applied Deep Learning using TensorFlow 2.0 and GCP (includes topics like data pipeline optimization, cyclical learning rates, mixed-precision training and so on) at Launchpad India Accelerator Bootcamp (December 12 - 13, 2019). Content available here: http://bit.ly/mlb-code-sayak.\nSpoke at DevLoop on Your first machine learning project, Ganpat University, Gujrat, India (January 04, 2020). Deck: http://bit.ly/dloop20. I have spoken about this topic at multiple occasions. A session recording is available here.\nSpoke at Improving machine learning model on Weights and Biases for better machine learning, Bangalore (February 08, 2020). Deck: http://bit.ly/blr-wb.\nSpoke at Sigma 2020 on Machine Learning: For the Community by the Community, Kolkata (February 12, 2020).\nSpoke at MENA Digital Days 2020 on Building data pipelines with tf.data. Deck here, session video here.\nSpoke for GDG Goa at an online event on Hello, TensorFlow. Deck here, session here.\nSpoke for GDG Pune and WTM Pune on Doing more with TensorFlow Lite, April 26, 2020. A session recording is available here (it was for the Deep Learning Salon hosted by Weights and Biases).\nSpoke for Global AI Hub, Turkey on Gotchas of Transfer Learning for Image Classification, May 01, 2020. A recording of the session is available here.\nSpoke on TensorFlow Hub: Models, Models, and Models for TFUG Hyderabad on May 03, 2020. Deck: http://bit.ly/tf-hub. A recording of the session is available here.\nParticipated in an ML fire-side chat hosted by Aniedi to speak to the developers of African regions. Recording is available here (May 30, 2020).\nSpoke at Pie and AI Kolkata on Becoming One with the Data. Deck here & session recording here (May 31, 2020).\nSpoke on Model Optimization 101 for TFUG Thrissur. Deck here (June 07, 2020). A recording on the same topic from a different event is available here. I have also presented this at Google for Startups Accelerator India (GoogleDevsIN tweet). A recording of that session is available here.\nSpoke on A few good stuff in TensorFlow Lite for GDG Berlin (July 16, 2020). Deck is available here and a recording is available here.\nSpoke on GitHub Actions for Machine Learning at Global AI On Tour Mumbai (September 27, 2020). Deck is available here.\nSpoke on The Maker Philosophy with ML APIs at Mindhack Summit (October 12, 2020). Deck is available here.\nSpoke on Adversarial Examples in Deep Learning at DevFest UK & Ireland 2020 (October 17, 2020). Deck is available here.\nTook a workshop on Adversarial Robustness in Deep Learning with Dipanjan Sarkar at Deep Learning DevCon 2020 (October 29, 2020). The materials are available here - bit.ly/adv_learn. We presented this at Kshitij 2021 as well and here‚Äôs the recording.\nSpoke on Demystifying Self-Supervised Learning for Visual Recognition at SciPy Japan 2020 (October 30, 2020). Deck is available here - bit.ly/scipy-sp. Session recording is available here.\nSpoke to Santosh on Demystifying ML and AI for beginners at his podcast Tech Talks With Santosh: https://youtu.be/BhLQN-XIO04 (November 07, 2020).\nSpoke on Key Trends of Computer Vision 2021 (June 09, 2021). Deck is available here. A session recording is available here. A blog post summarizing the content (by Benedict Neo) is available here. I have also presented this at Takshak‚Äô21, Techfest of ISM Dhanbad.\nVertex AI for Easier Model Deployments at Google for Startups Accelerator Class 5 (August 28, 2021). Deck is available here. A recording is available here.\nEnabling Possibilities with Open-Source in Machine Learning at DevFest India 2021 (October 24, 2021). My deck is available here. A recording is available here.\nCitizen Scientists Tackling Devastating Floods and Disaster Relief with Semi-supervised Deep Learning at NVIDIA GTC 2021 (November 09, 2021). This is based on our work on using semi-supervision for segmenting flood regions. This work was also presented as a poster at PyTorch Developer Day and the PyTorch team helped us create a beautiful poster for that.\nImproving Dataflow pipelines for text data at scale (with Nilabhra Roy Chowdhury) at Innovators Hive from Google Cloud (March 30, 2022). We also presented this at the Beam Summit 2022. Slide deck is available here. A video presentation is available here.\nBuilding a Mobile-optimized Image Recognition Model at DevFest Bangalore 2022 (September 18, 2022). Slides are here. A recording is here.\nFantastic ML Deployments & How to do them with Vertex AI at Cloud Community Days Ahmedabad 2022 (September 24, 2022). Slides are here.\nBetter Hardware Provisioning for ML Experiments on GCP at Cloud Next 2022 (October 13, 2022). All the materials are here. A video recording is available here.\nBeing a Maker with ML APIs at DevFest Jalandhar 2022 (October 16, 2022). All the materials are here.\nAnatomy of Capstone ML Projects at DevFest Nagpur 2022 (November 5, 2022). Slides are here. Presented this at DevFest Tashkent 2022 as well.\nImproving as an ML Practitioner at DevFest Kolkata 2022 (November 13, 2022). Slides are here. Presented this at DevFest Indore 2022 as well.\nBringing ML Models to Life with Hugging Face Spaces at DevFest Gandhinagar 2022 (November 21, 2022).\nML Problems: Formulation and Adoption at Google Developers Startup Bootcamp (University Edition); Gurugram, India (December 06, 2022). Tweet by GoogleDevsIN. Deck is here.\nYou Don‚Äôt Know TensorFlow at TFUG Kolkata (March 19, 2023). Materials are here. A recording of the talk can be found here.\nOpen and Collaborative MLOps, invited lecture at HKUST (March 31, 2023). My slides are here.\nBuilding DALL-E 2 like Systems in Minutes at NIT Silchar (April 8, 2023). Presented on the same topic at IIT Guwahati (April 22, 2023). Slides are here.\nDemocratizing Generative Models with üß® diffusers at the NVIDIA-HF meetup (April 29, 2023) (with Suraj Patil). Slides are here. Here‚Äôs another version of the presentation that I presented at the DataHack Summit 2023.\nShip Faster TensorFlow Models with XLA at Google Cloud Community Days, Kolkata (May 7, 2023). Slides are here.\n\nGenerating Photorealistic Images using AI with Diffusers in Python at DataCamp (May 16, 2023). Materials are here. A recording of the session is here.\nüß® diffusers for research at VAL, Indian Institute of Science (IISc) (June 12, 2023). Slides are available here.\nAppeared on a podcast discussing my ML journey and other related things: Hugging Face‚Äôs ML Revolution.\nMaking Keras Models go brrr with XLA at Keras Community Day Kolkata, August 27, 2023. Slides are here. I have given the same talk at Keras Community Day Chandigarh too (September 02, 2023).\nBringing SoTA Diffusion Models to the Masses with üß® diffusers, throughout September 2023. Presented this at the following places during my trip to the UK: The Dyson Robotics Lab at Imperial College of London, Amazon London, and the Department of Statistics at the University of Oxford. Slides are here. A recording (Oxford version) is available here.\nControlling Text-to-Image Diffusion Models, BigMAC ICCV Workshop (October 02, 2023). Slides are here. Recording is here.\nBringing SoTA Diffusion Models to the Masses with üß® diffusers, IBM Research (October 17, 2023). Slides are here. Recording is here.\nPeople of AI podcast (Google), spoke to Ashley Oldacre and Luiz Gustavo Martins from Google. Episode recording is here.\nEnd-to-End Pipeline for Segmentation with TFX, GCP, & ü§ó (with Chansung Park), ML Community Summit 2023 (November 23, 2023). Slides are here.\nSoftware Engineering Daily, spoke to Sean Falconer. Podcast recording is here.\nStaying relevant as an MLE in the age of GenAI, DevFest Kolkata 2023 (December 22, 2023). Slides are here.\nAppeared on the Overpowered podcast with Varun Mayya. Find it here.\nBuilding a Personal Coding Assistant (with Sourab Mangrulkar), Build with AI, Bangalore (March 02, 2024). Slides are here.\nAppeared on the Humans of Digital podcast by CI&T. Find it here.\nML Career in 2024, IIT Indore (April 11, 2024). Slides are here.\nA talk on diffusion models, ETH Zurich (May 06, 2024). Slides are here.\n\n\n\nCo-organized by me\n\nDevFest Kolkata, August 3, 2019.\nTensorFlow All-Around Kolkata, August 31, 2019.\nLet‚Äôs Build, January 4, 2020.\nKolkata Kreate, February 29, 2020.\nTFUG India Summit, September 3 - 6, 2020.\nGenerative AI Meet-up in collaboration with NVIDIA, April 29, 2023.\nJAX/Diffusers Sprint, March 29 - May 08, 2023.\nDeploying Generative AI Models in collaboration with Inferless and Sequoia Capital, June 10, 2023.\n\n\n\nML Street Talk\nI used to appear on one of the most popular podcasts in machine learning called the ML Street Talk. Below are the episodes where I have made appearances.\n\nSpoke to Tim Scarfe, Connor Shorten, and Yannic Kilcher.\nSpoke to Mathilde Caron (Research Assistant at Facebook AI), with Ayush Thakur, Tim Scarfe, and Yannic Kilcher.\nSpoke to Sara Hooker (Google Brain), with Tim Scarfe, and Yannic Kilcher.\nSpoke to Sanyam Bhutani (H2O.ai), with Tim Scarfe, Yannic Kilcher, and Alex Stenlake.\nSpoke to Simon Kornblith (Google Brain) with Tim Scarfe and Yannic Kilcher.\nSpoke to Lena Voita (University of Edinburgh and University of Amsterdam) with Tim Scarfe and Yannic Kilcher.\nSpoke to Hadi Salman (MIT) with Tim Scarfe and Yannic Kilcher.\nSpoke to Ishan Misra (Facebook AI Research) with Tim Scarfe and Yannic Kilcher.\n\nNote: If you are interested to invite me as a speaker for your event, please get in touch by dropping an email at spsayakpaul@gmail.com. If you are interested in having me submit a CFP first, that is absolutely fine! Please don‚Äôt hesitate to ask that."
  },
  {
    "objectID": "pages/interviews.html",
    "href": "pages/interviews.html",
    "title": "Interviews",
    "section": "",
    "text": "The purpose of conducting these interviews is to mainly get insights about the real-world project experiences, perspectives on learning new things, some fun facts and thereby enriching the communities in the process. I sincerely thank the interviewees for taking the time out from their busy schedules and for agreeing to do these interviews. Here are the interviews I have done so far -\n\nAn interview with Robert Crowe, Developer Advocate (TensorFlow) at Google\nAn interview with Snehasis Banerjee, Scientist at TCS Research and Innovation\nAn interview with Abhishek Kumar, Senior Manager, Data Science at Publicis Sapient\nAn interview with Laurence Moroney, Developer Advocate at Google\nAn interview with Karl Fezer, AI Ecosystem Evangelist at Arm\nAn interview with Dan Becker, Team Lead of Kaggle Learn & Product Lead of Kaggle Kernels\nAn interview with Rajarshee Mitra, Data Scientist at Microsoft\nAn interview with Alessio, Lead Data Scientist at FloydHub\nAn interview with Joel Grus, Research Engineer at Allen Institute for Artificial Intelligence\nAn interview with Josh Tobin, Research Scientist at OpenAI\nAn interview with Andrew Ferlitsch, Developer Program Engineer at Google\nAn interview with Shalini De Mello, Principal Research Scientist at NVIDIA\nAn interview with Rahul Agrawal, Principal Machine Learning Manager at AI and Research, Microsoft\nAn interview with Aakash Nain, Research Engineer at Ola\nAn interview with Xander Steenbrugge, Machine Learning Researcher & YouTuber at ‚ÄúArxiv Insights‚Äù\nAn interview with Ines Montani, Co-founder at Explosion\nAn interview with Girish Palshikar, Principal Scientist at TCS Research and Innovation\nAn interview with Christoph Molnar, Interpretable Machine Learning Researcher\nAn interview with Leslie Smith, Senior Research Scientist at U.S. Naval Research Laboratory\nAn interview with Arindam Pal, Senior Research Scientist at CSIRO\nAn interview with Ankur Patel, Vice President of Data Science at 7Park Data\nAn interview with Max Pumperla, Deep Learning Engineer at Skymind\nAn interview with Abhishek Thakur, Data Scientist, and Kaggle 3x Grandmaster\nAn interview with Dmytro Mishkin, Computer Vision Researcher\nAn interview with Ellick Chan, Head of University Relations and Research ‚Äî Intel AI Academy\nAn interview with Thomas Wolf, Chief Science Officer at Hugging Face\nAn interview with Dat Tran, Head of AI at Axel Springer AI\nAn interview with Daniel Seita, Ph.D.¬†student at UC, Berkeley\nAn interview with Vladimir Iglovikov, Senior Computer Vision Engineer at Lyft\nAn interview with Hamel Husain, Staff Machine Learning Engineer at GitHub\nAn interview with Patrick Hall, Principal Scientist at bnh.ai and Advisor to H2O.ai\nAn interview with Colin Raffel, Research Scientist at Google\nAn interview with Niki Parmar, Senior Research Scientist at Google Brain\nAn interview with Alexander (Sasha) Rush, Associate Professor at Cornell University\nAn interview with Vincent Sitzmann, Postdoctoral Researcher at MIT\nAn interview with Dan Hendrycks, Ph.D.¬†student at UC Berkeley\n\nI have had an amazing time interviewing these incredible folks and I am grateful to them. For other competing priorities, I am discontinuing the series for an indefinite period. If you enjoy reading through interviews like these, you might want to check out the Machine Learning Street Talk podcast on YouTube."
  },
  {
    "objectID": "pages/resources.html",
    "href": "pages/resources.html",
    "title": "Resources",
    "section": "",
    "text": "This page enlists the talks I have given, the podcasts I have appeared on and co-hosted, and other misc things.\n\nTalks / podcasts / events\nMentorship / Awards / Etc.\nInterviews\nA bit about my formal education"
  }
]